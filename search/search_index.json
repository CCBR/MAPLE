{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Background \u00b6 The MNaseSeq pipeline was developed in support of NIH's Dr. Zhurkin Laboratory . It has been developed and tested solely on NIH HPC Biowulf.","title":"Background"},{"location":"#background","text":"The MNaseSeq pipeline was developed in support of NIH's Dr. Zhurkin Laboratory . It has been developed and tested solely on NIH HPC Biowulf.","title":"Background"},{"location":"ccbr1214/contributions/","text":"Contributions \u00b6 The following members contributed to the development of the iCLIP pipeline: Wilfried Guiblet Samantha Sevilla Vishal Koparde Victor Zhurkin WG, SS, VK contributed to the generating the source code and all members contributed to the main concepts and analysis.","title":"Contributions"},{"location":"ccbr1214/contributions/#contributions","text":"The following members contributed to the development of the iCLIP pipeline: Wilfried Guiblet Samantha Sevilla Vishal Koparde Victor Zhurkin WG, SS, VK contributed to the generating the source code and all members contributed to the main concepts and analysis.","title":"Contributions"},{"location":"ccbr1214/getting-started/","text":"Overview \u00b6 The MNaseSeq github repository is stored locally, and will be used for project deployment. Multiple projects can be deployed from this one point simultaneously, without concern. 1. Getting Started\u00b6 \u00b6 1.1 Introduction \u00b6 The MNaseSeq Pipeline beings with raw FASTQ files and performs adaptor trimming, assembly, and alignment. Bed files are created, and depending on user input, selected regions of interst may be used. Fragment centers (DYAD's) are then determined, and histograms of occurences are created. QC reports are also generated with each project. The following are sub-commands used within MNaseSeq: init: initalize the pipeline dryrun: predict the binding of peptides to any MHC molecule run: execute the pipeline on the Biowulf HPC runlocal: execute a local, interactive, session unlock: unlock directory reset: delete a workdir, and re-initialize 1.2 Setup Dependencies\u00b6 \u00b6 MNaseSeq has several dependencies listed below. These dependencies can be installed by a sysadmin. All dependencies will be automatically loaded if running from Biowulf. bedtools: \"bedtools/2.30.0\" bowtie2: \"bowtie/2-2.4.2\" cutadapt: \"cutadapt/1.18\" pear: \"pear/0.9.11\" python: \"python/3.7\" R: \"R/4.0.3\" samtools: \"samtools/1.11\" 1.3 Login to the cluster\u00b6 \u00b6 MNaseSeq has been exclusively tested on Biowulf HPC. Login to the cluster's head node and move into the pipeline location. # ssh into cluster's head node ssh -Y $USER@biowulf.nih.gov 1.4 Load an interactive session\u00b6 \u00b6 An interactive session should be started before performing any of the pipeline sub-commands, even if the pipeline is to be executed on the cluster. # Grab an interactive node srun -N 1 -n 1 --time=12:00:00 -p interactive --mem=8gb --cpus-per-task=4 --pty bash","title":"1. Getting Started"},{"location":"ccbr1214/getting-started/#overview","text":"The MNaseSeq github repository is stored locally, and will be used for project deployment. Multiple projects can be deployed from this one point simultaneously, without concern.","title":"Overview"},{"location":"ccbr1214/getting-started/#1-getting-started","text":"","title":"1. Getting Started\u00b6"},{"location":"ccbr1214/getting-started/#11-introduction","text":"The MNaseSeq Pipeline beings with raw FASTQ files and performs adaptor trimming, assembly, and alignment. Bed files are created, and depending on user input, selected regions of interst may be used. Fragment centers (DYAD's) are then determined, and histograms of occurences are created. QC reports are also generated with each project. The following are sub-commands used within MNaseSeq: init: initalize the pipeline dryrun: predict the binding of peptides to any MHC molecule run: execute the pipeline on the Biowulf HPC runlocal: execute a local, interactive, session unlock: unlock directory reset: delete a workdir, and re-initialize","title":"1.1 Introduction"},{"location":"ccbr1214/getting-started/#12-setup-dependencies","text":"MNaseSeq has several dependencies listed below. These dependencies can be installed by a sysadmin. All dependencies will be automatically loaded if running from Biowulf. bedtools: \"bedtools/2.30.0\" bowtie2: \"bowtie/2-2.4.2\" cutadapt: \"cutadapt/1.18\" pear: \"pear/0.9.11\" python: \"python/3.7\" R: \"R/4.0.3\" samtools: \"samtools/1.11\"","title":"1.2 Setup Dependencies\u00b6"},{"location":"ccbr1214/getting-started/#13-login-to-the-cluster","text":"MNaseSeq has been exclusively tested on Biowulf HPC. Login to the cluster's head node and move into the pipeline location. # ssh into cluster's head node ssh -Y $USER@biowulf.nih.gov","title":"1.3 Login to the cluster\u00b6"},{"location":"ccbr1214/getting-started/#14-load-an-interactive-session","text":"An interactive session should be started before performing any of the pipeline sub-commands, even if the pipeline is to be executed on the cluster. # Grab an interactive node srun -N 1 -n 1 --time=12:00:00 -p interactive --mem=8gb --cpus-per-task=4 --pty bash","title":"1.4 Load an interactive session\u00b6"},{"location":"ccbr1214/output/","text":"4. Expected Outputs\u00b6 \u00b6 The following directories are created under the output_directory: 01_trim: this directory includes trimmed FASTQ files 02_assembled: this directory includes assembled FASTQ files 03_aligned: this directory includes aligned BAM files and BED files 01_bam: BAM files after alignment 02_bed: converted bed files 03_histograms: histograms of bed files 04_dyads: this directory contains DYAD calculated files 01_DYADs: this includes direct DYAD calculations 02_histograms: this includes histogram occurances 03_CSV: this includes the occurance data in CSV format qc: this directory includes the qc reports, sorted by: multiqc_report: this includes the fastqc results, as well as fastq screen results of each sample before and after filtering log: this includes log files [date of run]: the slurm output files of the pipeline sorted by pipeline start time; copies of config and manifest files used in this specific pipeline run; error reporting script","title":"4. Expected Output"},{"location":"ccbr1214/output/#4-expected-outputs","text":"The following directories are created under the output_directory: 01_trim: this directory includes trimmed FASTQ files 02_assembled: this directory includes assembled FASTQ files 03_aligned: this directory includes aligned BAM files and BED files 01_bam: BAM files after alignment 02_bed: converted bed files 03_histograms: histograms of bed files 04_dyads: this directory contains DYAD calculated files 01_DYADs: this includes direct DYAD calculations 02_histograms: this includes histogram occurances 03_CSV: this includes the occurance data in CSV format qc: this directory includes the qc reports, sorted by: multiqc_report: this includes the fastqc results, as well as fastq screen results of each sample before and after filtering log: this includes log files [date of run]: the slurm output files of the pipeline sorted by pipeline start time; copies of config and manifest files used in this specific pipeline run; error reporting script","title":"4. Expected Outputs\u00b6"},{"location":"ccbr1214/preparing-files/","text":"2. Preparing Files\u00b6 \u00b6 The pipeline is controlled through editing configuration and manifest files. Defaults are found in the /WORKDIR/ after initialization. 2.1 Configs\u00b6 \u00b6 The configuration files control parameters and software of the pipeline. These files are listed below: resources/cluster.yaml resources/tools.yaml config.yaml 2.1.1 Cluster YAML\u00b6 \u00b6 The cluster configuration file dictates the resouces to be used during submission to Biowulf HPC. There are two differnt ways to control these parameters - first, to control the default settings, and second, to create or edit individual rules. These parameters should be edited with caution, after significant testing. 2.1.2 Tools YAML\u00b6 \u00b6 The tools configuration file dictates the version of each tool that is being used. Updating the versions may break specific rules if versions are not backwards compatible with the defaults listed. 2.1.3 FASTQ Screen Config\u00b6 \u00b6 The FASTQ screen configuration files dictates the parameters used for FASTQ Screen. 2.1.4 MultiQC Config\u00b6 \u00b6 The MultiQC screen configuration files control the parameters used for MultiQC. 2.1.5 Config YAML\u00b6 \u00b6 There are several groups of parameters that are editable for the user to control the various aspects of the pipeline. These are : Folders and Paths These parameters will include the input and ouput files of the pipeline, as well as list all manifest names. User parameters These parameters will control the pipeline features. These include thresholds and whether to perform processes. 2.2 Preparing Manifests\u00b6 \u00b6 There are two manifests which are required for the pipeline. These files describe information on the samples and desired contrasts. The paths of these files are defined in the config.yaml file. These files are: sampleManifest contrastManifest 2.2.1 Samples Manifest\u00b6 \u00b6 This manifest will include information to sample level information. It includes the following column headers: sampleName type path_to_R1_fastq path_to_R2_fastq sampleName: the sampleID associated with the fasta file; which are unique. This may be a shorthand name, and will be used throughout the analysis. type: demographic information regarding the sample; example 'tumor' path_to_R1_fastq: the full path to the R1.fastq.gz file path_to_R1_fastq: the full path to the R2.fastq.gz file An example sampleManifest file with multiplexing of one sample. Notice that the multiplexID test_1 is repeated, as Ro_Clip and Control_Clip are both found in the same fastq file, whereas test_2 is not multiplexed: sampleName type path_to_R1_fastq path_to_R2_fastq Sample1 tumor /path/to/sample1.R1.fastq.gz /path/to/sample1.R2.fastq.gz Sample2 tumor /path/to/sample2.R1.fastq.gz /path/to/sample2.R2.fastq.gz Sample3 tumor /path/to/sample3.R1.fastq.gz /path/to/sample3.R2.fastq.gz Sample4 tumor /path/to/sample4.R1.fastq.gz /path/to/sample4.R2.fastq.gz 2.2.2 Contrast Manifest\u00b6 \u00b6 This manifest will include sample level information to compare samples. - contrast1: the sample name, identified in the samplesManifest [sampleName] column, of the sample to compare. example: 'Sample1' - contrast2: the sample name, identified in the samplesManifest [sampleName] column, of the sample to compare. example: 'Sample2' Additional columns may be added, however, the same number of comparisons must be performed each time. For example, manifest 1 would be accepted, but manifest 2 would fail, as row 1 has three comparisons and row 2 has four comparisons. Manifest example 1 (PASS) contrast1 contrast2 contrast3 Sample1 Sample2 Sample3 Sample2 Sample3 Sample4 Manifest example 2 (FAIL) contrast1 contrast2 contrast3 contrast4 Sample1 Sample2 Sample3 Sample2 Sample3 Sample4 Sample1","title":"2. Preparing Files"},{"location":"ccbr1214/preparing-files/#2-preparing-files","text":"The pipeline is controlled through editing configuration and manifest files. Defaults are found in the /WORKDIR/ after initialization.","title":"2. Preparing Files\u00b6"},{"location":"ccbr1214/preparing-files/#21-configs","text":"The configuration files control parameters and software of the pipeline. These files are listed below: resources/cluster.yaml resources/tools.yaml config.yaml","title":"2.1 Configs\u00b6"},{"location":"ccbr1214/preparing-files/#211-cluster-yaml","text":"The cluster configuration file dictates the resouces to be used during submission to Biowulf HPC. There are two differnt ways to control these parameters - first, to control the default settings, and second, to create or edit individual rules. These parameters should be edited with caution, after significant testing.","title":"2.1.1 Cluster YAML\u00b6"},{"location":"ccbr1214/preparing-files/#212-tools-yaml","text":"The tools configuration file dictates the version of each tool that is being used. Updating the versions may break specific rules if versions are not backwards compatible with the defaults listed.","title":"2.1.2 Tools YAML\u00b6"},{"location":"ccbr1214/preparing-files/#213-fastq-screen-config","text":"The FASTQ screen configuration files dictates the parameters used for FASTQ Screen.","title":"2.1.3 FASTQ Screen Config\u00b6"},{"location":"ccbr1214/preparing-files/#214-multiqc-config","text":"The MultiQC screen configuration files control the parameters used for MultiQC.","title":"2.1.4 MultiQC Config\u00b6"},{"location":"ccbr1214/preparing-files/#215-config-yaml","text":"There are several groups of parameters that are editable for the user to control the various aspects of the pipeline. These are : Folders and Paths These parameters will include the input and ouput files of the pipeline, as well as list all manifest names. User parameters These parameters will control the pipeline features. These include thresholds and whether to perform processes.","title":"2.1.5 Config YAML\u00b6"},{"location":"ccbr1214/preparing-files/#22-preparing-manifests","text":"There are two manifests which are required for the pipeline. These files describe information on the samples and desired contrasts. The paths of these files are defined in the config.yaml file. These files are: sampleManifest contrastManifest","title":"2.2 Preparing Manifests\u00b6"},{"location":"ccbr1214/preparing-files/#221-samples-manifest","text":"This manifest will include information to sample level information. It includes the following column headers: sampleName type path_to_R1_fastq path_to_R2_fastq sampleName: the sampleID associated with the fasta file; which are unique. This may be a shorthand name, and will be used throughout the analysis. type: demographic information regarding the sample; example 'tumor' path_to_R1_fastq: the full path to the R1.fastq.gz file path_to_R1_fastq: the full path to the R2.fastq.gz file An example sampleManifest file with multiplexing of one sample. Notice that the multiplexID test_1 is repeated, as Ro_Clip and Control_Clip are both found in the same fastq file, whereas test_2 is not multiplexed: sampleName type path_to_R1_fastq path_to_R2_fastq Sample1 tumor /path/to/sample1.R1.fastq.gz /path/to/sample1.R2.fastq.gz Sample2 tumor /path/to/sample2.R1.fastq.gz /path/to/sample2.R2.fastq.gz Sample3 tumor /path/to/sample3.R1.fastq.gz /path/to/sample3.R2.fastq.gz Sample4 tumor /path/to/sample4.R1.fastq.gz /path/to/sample4.R2.fastq.gz","title":"2.2.1 Samples Manifest\u00b6"},{"location":"ccbr1214/preparing-files/#222-contrast-manifest","text":"This manifest will include sample level information to compare samples. - contrast1: the sample name, identified in the samplesManifest [sampleName] column, of the sample to compare. example: 'Sample1' - contrast2: the sample name, identified in the samplesManifest [sampleName] column, of the sample to compare. example: 'Sample2' Additional columns may be added, however, the same number of comparisons must be performed each time. For example, manifest 1 would be accepted, but manifest 2 would fail, as row 1 has three comparisons and row 2 has four comparisons. Manifest example 1 (PASS) contrast1 contrast2 contrast3 Sample1 Sample2 Sample3 Sample2 Sample3 Sample4 Manifest example 2 (FAIL) contrast1 contrast2 contrast3 contrast4 Sample1 Sample2 Sample3 Sample2 Sample3 Sample4 Sample1","title":"2.2.2 Contrast Manifest\u00b6"},{"location":"ccbr1214/run/","text":"3. Running the Pipeline\u00b6 \u00b6 3.1 Pipeline Overview\u00b6 \u00b6 The Snakemake workflow has a multiple options: Usage: bash ${SCRIPTNAME} -m/--runmode=<RUNMODE> -w/--workdir=<WORKDIR> Required Arguments: 1. RUNMODE: [Type: String] Valid options: *) init : initialize workdir *) run : run with slurm *) reset : DELETE workdir dir and re-init it *) dryrun : dry run snakemake to generate DAG *) unlock : unlock workdir if locked by snakemake *) runlocal : run without submitting to sbatch 2. WORKDIR: [Type: String]: Absolute or relative path to the output folder with write permissions. 3.2 Commands explained\u00b6 \u00b6 The following explains each of the command options: Preparation Commands - init (REQUIRED): This must be performed before any Snakemake run (dry, local, cluster) can be performed. This will copy the necessary config, manifest and Snakefiles needed to run the pipeline to the provided output directory. - dryrun (OPTIONAL): This is an optional step, to be performed before any Snakemake run (local, cluster). This will check for errors within the pipeline, and ensure that you have read/write access to the files needed to run the full pipeline. Processing Commands - runlocal - This will run the pipeline on a local node. NOTE: This should only be performed on an interactive node. - run - This will submit a master job to the cluster, and subsequent sub-jobs as needed to complete the workflow. An email will be sent when the pipeline begins, if there are any errors, and when it completes. Other Commands (All optional) - unlock: This will unlock the pipeline if an error caused it to stop in the middle of a run. - reset: This will DELETE workdir dir and re-init it To run any of these commands, follow the the syntax: ./run --runmode=COMMAND --workdir=/path/to/output/dir 3.3 Typical Workflow\u00b6 \u00b6 A typical command workflow, running on the cluser, is as follows: ./run --runmode=init --workdir=/path/to/output/dir ./run --runmode=dryrun --workdir=/path/to/output/dir ./run --runmode=run --workdir=/path/to/output/dir","title":"3. Running the Pipeline"},{"location":"ccbr1214/run/#3-running-the-pipeline","text":"","title":"3. Running the Pipeline\u00b6"},{"location":"ccbr1214/run/#31-pipeline-overview","text":"The Snakemake workflow has a multiple options: Usage: bash ${SCRIPTNAME} -m/--runmode=<RUNMODE> -w/--workdir=<WORKDIR> Required Arguments: 1. RUNMODE: [Type: String] Valid options: *) init : initialize workdir *) run : run with slurm *) reset : DELETE workdir dir and re-init it *) dryrun : dry run snakemake to generate DAG *) unlock : unlock workdir if locked by snakemake *) runlocal : run without submitting to sbatch 2. WORKDIR: [Type: String]: Absolute or relative path to the output folder with write permissions.","title":"3.1 Pipeline Overview\u00b6"},{"location":"ccbr1214/run/#32-commands-explained","text":"The following explains each of the command options: Preparation Commands - init (REQUIRED): This must be performed before any Snakemake run (dry, local, cluster) can be performed. This will copy the necessary config, manifest and Snakefiles needed to run the pipeline to the provided output directory. - dryrun (OPTIONAL): This is an optional step, to be performed before any Snakemake run (local, cluster). This will check for errors within the pipeline, and ensure that you have read/write access to the files needed to run the full pipeline. Processing Commands - runlocal - This will run the pipeline on a local node. NOTE: This should only be performed on an interactive node. - run - This will submit a master job to the cluster, and subsequent sub-jobs as needed to complete the workflow. An email will be sent when the pipeline begins, if there are any errors, and when it completes. Other Commands (All optional) - unlock: This will unlock the pipeline if an error caused it to stop in the middle of a run. - reset: This will DELETE workdir dir and re-init it To run any of these commands, follow the the syntax: ./run --runmode=COMMAND --workdir=/path/to/output/dir","title":"3.2 Commands explained\u00b6"},{"location":"ccbr1214/run/#33-typical-workflow","text":"A typical command workflow, running on the cluser, is as follows: ./run --runmode=init --workdir=/path/to/output/dir ./run --runmode=dryrun --workdir=/path/to/output/dir ./run --runmode=run --workdir=/path/to/output/dir","title":"3.3 Typical Workflow\u00b6"},{"location":"ccbr1214/test/","text":"Pipeline Tutorial\u00b6 Welcome to the MNaseSeq Pipeline Tutorial! 5. Pipeline Tutorial \u00b6 5.1 Getting Started\u00b6 \u00b6 Review the information on the Getting Started for a complete overview the pipeline. The tutorial below will use test data available on NIH Biowulf HPC only. All example code will assume you are running v1.0 of the pipeline, from the shared [tobedetermined] storage directory, using test_1 data. A. Change working directory to the iCLIP repository # general format cd /path/to/pipeline/[version number] # example cd /path/to/pipeline/v1.0 B. Initialize Pipeline ./run --runmode=init --workdir=/path/to/output/dir 5.2 Prepare the test set\u00b6 \u00b6 A. Two different test data sets are available, depending on the need. These include: - test_1: Single sample comparison (two samples) - test_2: Triple sample comparison (three samples) B. Pull the test data to your output directory NOTE: Test data is currently available for v1.0. Please contact samantha.sevilla@nih.gov to create other test data. # general format sh /data/CCBR_Pipeliner/ccbr1214/test/run_test.sh \\ -t test_number \\ -v version_id \\ -o /path/to/output/dir # example running test_1, v1.0: sh /data/CCBR_Pipeliner/ccbr1214/test/run_test.sh \\ -t test_1 \\ -v v1.0 \\ -o /path/to/output/dir 5.3 Complete dry-run\u00b6 \u00b6 A. Complete a dry-run and review output ./run --runmode=dryrun --workdir=/path/to/output/dir Ensure that an expected output is displayed. An expected output for test_1 is as follows: job count min threads max threads ------------- ------- ------------- ------------- alignment 2 48 48 all 1 1 1 assembly 2 48 48 dyad_analysis 2 48 48 hist_frags 2 48 48 trim_adaptors 2 48 48 total 11 1 48 An expected output for test_2 is as follows: 5.4 Run the pipeline\u00b6 \u00b6 Execute pipeline on the cluster #submit to the cluster ./run --runmode=run --workdir=/path/to/output/dir 5.5 Review outputs\u00b6 \u00b6 Review the expected outputs on the Output page. If there are errors, review and performing stesp described on the Troubleshooting page as needed. 5.6 Expert User Test Run \u00b6 An example of a test workflow is shown below for test_1, v1.0 of the pipeline source_dir=\"/home/sevillas2/git/ccbr1214\";\\ output_dir=\"/data/sevillas2/ccbr1214\";\\ test_id=\"test_1\";\\ version_id=\"v1.0\";\\ output_dir_full=${output_dir}/${version_id};\\ if [[ -d $output_dir_full ]]; then rm -r $output_dir_full; fi ;\\ cd $source_dir;\\ ./run --runmode=init --workdir=$output_dir_full;\\ sh /data/CCBR_Pipeliner/ccbr1214/test/run_test.sh -t $test_id -v $version_id -o $output_dir_full;\\ ./run --runmode=dryrun --workdir=$output_dir_full; ./run --runmode=run --workdir=$output_dir_full;","title":"5. Running Example Data"},{"location":"ccbr1214/test/#5-pipeline-tutorial","text":"","title":"5. Pipeline Tutorial"},{"location":"ccbr1214/test/#51-getting-started","text":"Review the information on the Getting Started for a complete overview the pipeline. The tutorial below will use test data available on NIH Biowulf HPC only. All example code will assume you are running v1.0 of the pipeline, from the shared [tobedetermined] storage directory, using test_1 data. A. Change working directory to the iCLIP repository # general format cd /path/to/pipeline/[version number] # example cd /path/to/pipeline/v1.0 B. Initialize Pipeline ./run --runmode=init --workdir=/path/to/output/dir","title":"5.1 Getting Started\u00b6"},{"location":"ccbr1214/test/#52-prepare-the-test-set","text":"A. Two different test data sets are available, depending on the need. These include: - test_1: Single sample comparison (two samples) - test_2: Triple sample comparison (three samples) B. Pull the test data to your output directory NOTE: Test data is currently available for v1.0. Please contact samantha.sevilla@nih.gov to create other test data. # general format sh /data/CCBR_Pipeliner/ccbr1214/test/run_test.sh \\ -t test_number \\ -v version_id \\ -o /path/to/output/dir # example running test_1, v1.0: sh /data/CCBR_Pipeliner/ccbr1214/test/run_test.sh \\ -t test_1 \\ -v v1.0 \\ -o /path/to/output/dir","title":"5.2 Prepare the test set\u00b6"},{"location":"ccbr1214/test/#53-complete-dry-run","text":"A. Complete a dry-run and review output ./run --runmode=dryrun --workdir=/path/to/output/dir Ensure that an expected output is displayed. An expected output for test_1 is as follows: job count min threads max threads ------------- ------- ------------- ------------- alignment 2 48 48 all 1 1 1 assembly 2 48 48 dyad_analysis 2 48 48 hist_frags 2 48 48 trim_adaptors 2 48 48 total 11 1 48 An expected output for test_2 is as follows:","title":"5.3 Complete dry-run\u00b6"},{"location":"ccbr1214/test/#54-run-the-pipeline","text":"Execute pipeline on the cluster #submit to the cluster ./run --runmode=run --workdir=/path/to/output/dir","title":"5.4 Run the pipeline\u00b6"},{"location":"ccbr1214/test/#55-review-outputs","text":"Review the expected outputs on the Output page. If there are errors, review and performing stesp described on the Troubleshooting page as needed.","title":"5.5 Review outputs\u00b6"},{"location":"ccbr1214/test/#56-expert-user-test-run","text":"An example of a test workflow is shown below for test_1, v1.0 of the pipeline source_dir=\"/home/sevillas2/git/ccbr1214\";\\ output_dir=\"/data/sevillas2/ccbr1214\";\\ test_id=\"test_1\";\\ version_id=\"v1.0\";\\ output_dir_full=${output_dir}/${version_id};\\ if [[ -d $output_dir_full ]]; then rm -r $output_dir_full; fi ;\\ cd $source_dir;\\ ./run --runmode=init --workdir=$output_dir_full;\\ sh /data/CCBR_Pipeliner/ccbr1214/test/run_test.sh -t $test_id -v $version_id -o $output_dir_full;\\ ./run --runmode=dryrun --workdir=$output_dir_full; ./run --runmode=run --workdir=$output_dir_full;","title":"5.6 Expert User Test Run"},{"location":"ccbr1214/troubleshooting/","text":"Troubleshooting \u00b6 Recommended steps to troubleshoot the pipeline. 1.1 Email \u00b6 Check your email for an email regarding pipeline failure. You will receive an email from slurm@biowulf.nih.gov with the subject: Slurm Job_id=[#] Name=ccbr1214 Failed, Run time [time], FAILED, ExitCode 1 1.2 Error Report \u00b6 Run the error report script cd /[output_dir]/log/[time_of_run] sh 00_create_error_report.sh cat error.log Review the report for the rules that erred, and the sample information. An example report is listed below: The following error(s) were found in rules: ********************************************* Error in rule rule1: Error in rule rule2: Error in rule rule3: The following samples are affected by memory and must be deleted: rule1.[sbatchid].sp=[sample_name].err:[E::hts_open_format] Disc quota exceeded The following samples are affected by missing input files/output dir and should be reviewed: rule2.[sbatchid].sp=[sample_name].err:[E::hts_open_format] Failed to open file \"[file_name]\" : No such file or directory The following samples are affected by other error_rules and should be reviewed: rule3.[sbatchid].sp=[sample_name].err:[E::hts_open_format] TIMEOUT 1.3 Restart the run\u00b6 \u00b6 After addressing the issue, unlock the output directory, perform another dry-run and check the status of the pipeline, then resubmit to the cluster. #unlock dir ./run --runmode=unlock --workdir=/path/to/output/dir #perform dry-run ./run --runmode=dryrun --workdir=/path/to/output/dir #submit to cluster ./run --runmode=run --workdir=/path/to/output/dir","title":"Troubleshooting"},{"location":"ccbr1214/troubleshooting/#troubleshooting","text":"Recommended steps to troubleshoot the pipeline.","title":"Troubleshooting"},{"location":"ccbr1214/troubleshooting/#11-email","text":"Check your email for an email regarding pipeline failure. You will receive an email from slurm@biowulf.nih.gov with the subject: Slurm Job_id=[#] Name=ccbr1214 Failed, Run time [time], FAILED, ExitCode 1","title":"1.1 Email"},{"location":"ccbr1214/troubleshooting/#12-error-report","text":"Run the error report script cd /[output_dir]/log/[time_of_run] sh 00_create_error_report.sh cat error.log Review the report for the rules that erred, and the sample information. An example report is listed below: The following error(s) were found in rules: ********************************************* Error in rule rule1: Error in rule rule2: Error in rule rule3: The following samples are affected by memory and must be deleted: rule1.[sbatchid].sp=[sample_name].err:[E::hts_open_format] Disc quota exceeded The following samples are affected by missing input files/output dir and should be reviewed: rule2.[sbatchid].sp=[sample_name].err:[E::hts_open_format] Failed to open file \"[file_name]\" : No such file or directory The following samples are affected by other error_rules and should be reviewed: rule3.[sbatchid].sp=[sample_name].err:[E::hts_open_format] TIMEOUT","title":"1.2 Error Report"},{"location":"ccbr1214/troubleshooting/#13-restart-the-run","text":"After addressing the issue, unlock the output directory, perform another dry-run and check the status of the pipeline, then resubmit to the cluster. #unlock dir ./run --runmode=unlock --workdir=/path/to/output/dir #perform dry-run ./run --runmode=dryrun --workdir=/path/to/output/dir #submit to cluster ./run --runmode=run --workdir=/path/to/output/dir","title":"1.3 Restart the run\u00b6"}]}