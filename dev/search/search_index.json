{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Background","text":"<p>MAPLE ([M]NaseSeq [A]nalysis [P]ipe[l]i[n]e) was developed in support of NIH's Dr. Zhurkin Laboratory. It has been developed and tested solely on NIH HPC Biowulf.</p>"},{"location":"user-guide/contributions/","title":"Contributions","text":"<p>The following members contributed to the development of the MNaseSeq pipeline:</p> <ul> <li>Wilfried Guiblet</li> <li>Samantha Sevilla</li> <li>Vishal Koparde</li> <li>Victor Zhurkin</li> </ul> <p>WG, SS, VK contributed to the generating the source code and all members contributed to the main concepts and analysis.</p>"},{"location":"user-guide/getting-started/","title":"Overview","text":"<p>The MAPLE (**M**NaseSeq **A**nalysis **P**ipe**l**i**n**e) github repository is stored locally, and will be used for project deployment. Multiple projects can be deployed from this one point simultaneously, without concern.</p>"},{"location":"user-guide/getting-started/#1-getting-started","title":"1. Getting Started","text":""},{"location":"user-guide/getting-started/#11-introduction","title":"1.1 Introduction","text":"<p>MAPLE beings with raw FASTQ files and performs adaptor trimming, assembly, and alignment. Bed files are created, and depending on user input, selected regions of interst may be used. Fragment centers (DYAD's) are then determined, and histograms of occurences are created. QC reports are also generated with each project.</p> <p>The following are sub-commands used within MNaseSeq:</p> <ul> <li>init: initalize the pipeline</li> <li>dryrun: predict the binding of peptides to any MHC molecule</li> <li>run: execute the pipeline on the Biowulf HPC</li> <li>runlocal: execute a local, interactive, session</li> <li>unlock: unlock directory</li> <li>reset: delete a workdir, and re-initialize</li> </ul>"},{"location":"user-guide/getting-started/#12-setup-dependencies","title":"1.2 Setup Dependencies","text":"<p>MNaseSeq has several dependencies listed below. These dependencies can be installed by a sysadmin. All dependencies will be automatically loaded if running from Biowulf.</p> <ul> <li>bedtools: \"bedtools/2.30.0\"</li> <li>bowtie2: \"bowtie/2-2.4.2\"</li> <li>cutadapt: \"cutadapt/1.18\"</li> <li>pear: \"pear/0.9.11\"</li> <li>python: \"python/3.7\"</li> <li>R: \"R/4.0.3\"</li> <li>samtools: \"samtools/1.11\"</li> </ul>"},{"location":"user-guide/getting-started/#13-login-to-the-cluster","title":"1.3 Login to the cluster","text":"<p>MAPLE has been exclusively tested on Biowulf HPC. Login to the cluster's head node and move into the pipeline location. <pre><code># ssh into cluster's head node\nssh -Y $USER@biowulf.nih.gov\n</code></pre></p>"},{"location":"user-guide/getting-started/#14-load-an-interactive-session","title":"1.4 Load an interactive session","text":"<p>An interactive session should be started before performing any of the pipeline sub-commands, even if the pipeline is to be executed on the cluster. <pre><code># Grab an interactive node\nsrun -N 1 -n 1 --time=12:00:00 -p interactive --mem=8gb  --cpus-per-task=4 --pty bash\n</code></pre></p>"},{"location":"user-guide/output/","title":"4. Expected Outputs","text":"<p>The following directories are created under the output_directory, dependent on the Pass of the pipeline</p>"},{"location":"user-guide/output/#first-pass-first_pass","title":"First Pass (first_pass)","text":"<ul> <li>01_trim: this directory includes trimmed FASTQ files</li> <li>02_assembled: this directory includes assembled FASTQ files</li> <li>03_aligned: this directory includes aligned BAM files and BED files<ul> <li>01_bam: BAM files after alignment</li> <li>02_bed: converted bed files</li> <li>03_histograms: histograms of bed files</li> </ul> </li> </ul>"},{"location":"user-guide/output/#second-pass-second_pass","title":"Second Pass (second_pass)","text":"<ul> <li>04_dyads: this directory contains DYAD calculated files<ul> <li>01_DYADs: this includes direct DYAD calculations</li> <li>02_histograms: this includes histogram occurances</li> <li>03_CSV: this includes the occurance data in CSV format</li> </ul> </li> </ul>"},{"location":"user-guide/output/#third-pass-third_pass","title":"Third Pass (third_pass)","text":"<ul> <li>/path/to/output/contrast: this includes the contrast file for each sample provided in the contrasts.tsv manifest</li> </ul>"},{"location":"user-guide/output/#all-passes","title":"All Passes","text":"<ul> <li>log: this includes log files<ul> <li>[date of run]: the slurm output files of the pipeline sorted by pipeline start time; copies of config and manifest files used in this specific pipeline run; error reporting script</li> </ul> </li> </ul>"},{"location":"user-guide/preparing-files/","title":"2. Preparing Files","text":"<p>The pipeline is controlled through editing configuration and manifest files. Defaults are found in the /WORKDIR/ after initialization.</p>"},{"location":"user-guide/preparing-files/#21-configs","title":"2.1 Configs","text":"<p>The configuration files control parameters and software of the pipeline. These files are listed below:</p> <ul> <li>resources/cluster.yaml</li> <li>resources/tools.yaml</li> <li>config.yaml</li> </ul>"},{"location":"user-guide/preparing-files/#211-cluster-yaml-required","title":"2.1.1 Cluster YAML (REQUIRED)","text":"<p>The cluster configuration file dictates the resouces to be used during submission to Biowulf HPC. There are two differnt ways to control these parameters - first, to control the default settings, and second, to create or edit individual rules. These parameters should be edited with caution, after significant testing.</p>"},{"location":"user-guide/preparing-files/#212-tools-yaml-required","title":"2.1.2 Tools YAML (REQUIRED)","text":"<p>The tools configuration file dictates the version of each tool that is being used. Updating the versions may break specific rules if versions are not backwards compatible with the defaults listed.</p>"},{"location":"user-guide/preparing-files/#213-config-yaml-required","title":"2.1.3 Config YAML (REQUIRED)","text":"<p>There are several groups of parameters that are editable for the user to control the various aspects of the pipeline. These are :</p> <ul> <li>Folders and Paths<ul> <li>These parameters will include the input and ouput files of the pipeline, as well as list all manifest names.</li> </ul> </li> <li>User parameters<ul> <li>These parameters will control the pipeline features. These include thresholds and whether to perform processes.</li> </ul> </li> </ul>"},{"location":"user-guide/preparing-files/#22-preparing-manifests","title":"2.2 Preparing Manifests","text":"<p>There are two manifests used for the pipeline. These files describe information on the samples and desired contrasts. The paths of these files are defined in the config.yaml file. These files are:</p> <ul> <li>sampleManifest (REQUIRED for all Passes)</li> <li>contrastManifest (REQUIRED for third_pass)</li> </ul>"},{"location":"user-guide/preparing-files/#221-samples-manifest","title":"2.2.1 Samples Manifest","text":"<p>This manifest will include information to sample level information. It includes the following column headers: sampleName type path_to_R1_fastq path_to_R2_fastq</p> <ul> <li>sampleName: the sampleID associated with the fasta file; which are unique. This may be a shorthand name, and will be used throughout the analysis.</li> <li>type: demographic information regarding the sample; example 'tumor'</li> <li>path_to_R1_fastq: the full path to the R1.fastq.gz file</li> <li>path_to_R1_fastq: the full path to the R2.fastq.gz file</li> </ul> <p>An example sampleManifest file with multiplexing of one sample. Notice that the multiplexID test_1 is repeated, as Ro_Clip and Control_Clip are both found in the same fastq file, whereas test_2 is not multiplexed:</p> <pre><code>sampleName  type    path_to_R1_fastq                path_to_R2_fastq\nSample1     tumor   /path/to/sample1.R1.fastq.gz    /path/to/sample1.R2.fastq.gz\nSample2     tumor   /path/to/sample2.R1.fastq.gz    /path/to/sample2.R2.fastq.gz\nSample3     tumor   /path/to/sample3.R1.fastq.gz    /path/to/sample3.R2.fastq.gz\nSample4     tumor   /path/to/sample4.R1.fastq.gz    /path/to/sample4.R2.fastq.gz\n</code></pre>"},{"location":"user-guide/preparing-files/#222-contrast-manifest","title":"2.2.2 Contrast Manifest","text":"<p>This manifest will include contrast information of samples to compare. The first two Passes must be complete in order to run this final phase.</p> <p>Manifest example 1 (PASS) <pre><code>/path/to/RESULTSDIR/04_dyad/03_csv/sample1.hg19.140-160.DYAD_corrected.csv\n/path/to/RESULTSDIR/04_dyad/03_csv/sample2.hg19.140-160.DYAD_corrected.csv\n</code></pre></p> <p>This wil create the output file, dependent on the config inputs for <code>output_contrast_location</code> and the <code>selected_shorthand</code>: <pre><code>/path/to/output_contrast_location/final_sample1.sample2.140-160.selected_shorthand.DAC.csv\n</code></pre></p>"},{"location":"user-guide/run/","title":"3. Running the Pipeline","text":""},{"location":"user-guide/run/#31-pipeline-overview","title":"3.1 Pipeline Overview","text":"<p>The Snakemake workflow has a multiple options: <pre><code>Usage:\n    ./run -m/--runmode=&lt;RUNMODE&gt; -w/--workdir=&lt;WORKDIR&gt;\n\n    Required Arguments:\n    1.  RUNMODE: [Type: String] Valid options:\n        *) init : initialize workdir\n        *) run : run with slurm\n        *) reset : DELETE workdir dir and re-init it\n        *) dryrun : dry run snakemake to generate DAG\n        *) unlock : unlock workdir if locked by snakemake\n        *) runlocal : run without submitting to sbatch\n    2.  WORKDIR: [Type: String]: Absolute or relative path to the \n        output folder with write permissions.\n</code></pre></p>"},{"location":"user-guide/run/#32-commands-explained","title":"3.2 Commands explained","text":"<p>The following explains each of the command options:</p> <p>Preparation Commands</p> <ul> <li>init (REQUIRED): This must be performed before any Snakemake run (dry, local, cluster) can be performed. This will copy the necessary config, manifest and Snakefiles needed to run the pipeline to the provided output directory.</li> <li>dryrun (OPTIONAL): This is an optional step, to be performed before any Snakemake run (local, cluster). This will check for errors within the pipeline, and ensure that you have read/write access to the files needed to run the full pipeline.</li> </ul> <p>Processing Commands</p> <ul> <li>runlocal - This will run the pipeline on a local node. NOTE: This should only be performed on an interactive node.</li> <li>run - This will submit a master job to the cluster, and subsequent sub-jobs as needed to complete the workflow. An email will be sent when the pipeline begins, if there are any errors, and when it completes.</li> </ul> <p>Other Commands (All optional)</p> <ul> <li>unlock: This will unlock the pipeline if an error caused it to stop in the middle of a run.</li> <li>reset: This will DELETE workdir dir and re-init it</li> </ul> <p>To run any of these commands, follow the the syntax: <pre><code>./run --runmode=COMMAND --workdir=/path/to/output/dir\n</code></pre></p>"},{"location":"user-guide/run/#33-typical-workflow","title":"3.3 Typical Workflow","text":"<p>A typical command workflow, running on the cluser, is as follows: <pre><code>./run --runmode=init --workdir=/path/to/output/dir\n./run --runmode=dryrun --workdir=/path/to/output/dir\n./run --runmode=run --workdir=/path/to/output/dir\n</code></pre></p>"},{"location":"user-guide/run/#34-passes-explained","title":"3.4 Passes explained","text":"<p>MAPLE is to be run in three Passes: 1.) first_pass completes trimming, alignment, assembly and a complete histogram 2.) second_pass completes subsetting, DAC analysis and DYAD analysis 3.) third pass completes comparisons between multiple samples</p>"},{"location":"user-guide/test/","title":"5. Pipeline Tutorial","text":"<p>Welcome to the MNaseSeq Pipeline Tutorial!</p>"},{"location":"user-guide/test/#51-getting-started","title":"5.1 Getting Started","text":"<p>Review the information on the Getting Started for a complete overview the pipeline. The tutorial below will use test data available on NIH Biowulf HPC only. All example code will assume you are running v1.0 of the pipeline, from the shared [tobedetermined] storage directory, using test_1 data.</p> <p>A. Change working directory to the iCLIP repository <pre><code># general format\ncd /path/to/pipeline/[version number]\n\n# example\ncd /path/to/pipeline/v1.0\n</code></pre></p> <p>B. Initialize Pipeline <pre><code>./run --runmode=init --workdir=/path/to/output/dir\n</code></pre></p>"},{"location":"user-guide/test/#52-prepare-the-test-set","title":"5.2 Prepare the test set","text":"<p>A. Two different test data sets are available, depending on the need. These include: - test_1: Single sample comparison (two samples) - test_2: Triple sample comparison (three samples)</p> <p>B. Pull the test data to your output directory</p> <p>NOTE: Test data is currently available for v1.0. Please contact samantha.sevilla@nih.gov to create other test data.</p> <pre><code># general format\nsh /data/CCBR_Pipeliner/ccbr1214/test/run_test.sh \\\n    -t test_number \\\n    -v version_id \\\n    -o /path/to/output/dir\n\n# example running test_1, v1.0:\nsh /data/CCBR_Pipeliner/ccbr1214/test/run_test.sh \\\n    -t test_1 \\\n    -v v1.0 \\\n    -o /path/to/output/dir \n</code></pre>"},{"location":"user-guide/test/#53-complete-dry-run","title":"5.3 Complete dry-run","text":"<p>A. Complete a dry-run and review output <pre><code>./run --runmode=dryrun --workdir=/path/to/output/dir\n</code></pre> Ensure that an expected output is displayed. An expected output for test_1 is as follows: <pre><code>job              count    min threads    max threads\n-------------  -------  -------------  -------------\nalignment            2             48             48\nall                  1              1              1\nassembly             2             48             48\ndyad_analysis        2             48             48\nhist_frags           2             48             48\ntrim_adaptors        2             48             48\ntotal               11              1             48\n</code></pre></p> <p>An expected output for test_2 is as follows: <pre><code>\n</code></pre></p>"},{"location":"user-guide/test/#54-run-the-pipeline","title":"5.4 Run the pipeline","text":"<p>Execute pipeline on the cluster <pre><code>#submit to the cluster\n./run --runmode=run --workdir=/path/to/output/dir\n</code></pre></p>"},{"location":"user-guide/test/#55-review-outputs","title":"5.5 Review outputs","text":"<p>Review the expected outputs on the Output page. If there are errors, review and performing stesp described on the Troubleshooting page as needed.</p>"},{"location":"user-guide/test/#56-expert-user-test-run","title":"5.6 Expert User Test Run","text":"<p>An example of a test workflow is shown below for test_1, v1.0 of the pipeline <pre><code>source_dir=\"/home/sevillas2/git/ccbr1214\";\\\noutput_dir=\"/data/sevillas2/ccbr1214\";\\\ntest_id=\"test_1\";\\\nversion_id=\"v1.0\";\\\noutput_dir_full=${output_dir}/${version_id};\\\nif [[ -d $output_dir_full ]]; then rm -r $output_dir_full; fi ;\\\ncd $source_dir;\\\n./run --runmode=init --workdir=$output_dir_full;\\\nsh /data/CCBR_Pipeliner/ccbr1214/test/run_test.sh -t $test_id -v $version_id -o $output_dir_full;\\\n./run --runmode=dryrun --workdir=$output_dir_full;\n./run --runmode=run --workdir=$output_dir_full;\n</code></pre></p>"},{"location":"user-guide/troubleshooting/","title":"Troubleshooting","text":"<p>Recommended steps to troubleshoot the pipeline.</p>"},{"location":"user-guide/troubleshooting/#11-email","title":"1.1 Email","text":"<p>Check your email for an email regarding pipeline failure. You will receive an email from slurm@biowulf.nih.gov with the subject: Slurm Job_id=[#] Name=ccbr1214 Failed, Run time [time], FAILED, ExitCode 1</p>"},{"location":"user-guide/troubleshooting/#12-error-report","title":"1.2 Error Report","text":"<p>Run the error report script <pre><code>cd /[output_dir]/log/[time_of_run]\nsh 00_create_error_report.sh\ncat error.log\n</code></pre></p> <p>Review the report for the rules that erred, and the sample information. An example report is listed below: <pre><code>The following error(s) were found in rules:\n*********************************************\nError in rule rule1:\nError in rule rule2:\nError in rule rule3:\n\nThe following samples are affected by memory and must be deleted:\nrule1.[sbatchid].sp=[sample_name].err:[E::hts_open_format] Disc quota exceeded\n\nThe following samples are affected by missing input files/output dir and should be reviewed:\nrule2.[sbatchid].sp=[sample_name].err:[E::hts_open_format] Failed to open file \"[file_name]\" : No such file or directory\n\nThe following samples are affected by other error_rules and should be reviewed:\nrule3.[sbatchid].sp=[sample_name].err:[E::hts_open_format] TIMEOUT\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#13-restart-the-run","title":"1.3 Restart the run","text":"<p>After addressing the issue, unlock the output directory, perform another dry-run and check the status of the pipeline, then resubmit to the cluster.</p> <pre><code>#unlock dir\n./run --runmode=unlock --workdir=/path/to/output/dir\n\n#perform dry-run\n./run --runmode=dryrun --workdir=/path/to/output/dir\n\n#submit to cluster\n./run --runmode=run --workdir=/path/to/output/dir\n</code></pre>"}]}